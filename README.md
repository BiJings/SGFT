# SGFT
# Enhancing the Reasoning Capabilities of Small Language Models via Solution Guidance Fine-Tuning

This repository contains the official code of the paper: ["Enhancing the Reasoning Capabilities of Small Language Models via Solution Guidance Fine-Tuning"](https://arxiv.org/xx), accepted by The 31st International Conference on Computational Linguistics (COLING 2025).


### Citation
```
@article{geva2021strategyqa,
  title = {{Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies}},
  author = {Geva, Mor and Khashabi, Daniel and Segal, Elad and Khot, Tushar and Roth, Dan and Berant, Jonathan},
  journal = {Transactions of the Association for Computational Linguistics (TACL)},
  year = {2021},
}
```
### Solution-Guidance Fine-Tuning (SGFT)
Our paper introduce a new reasoning strategy Solution Guidance (SG) and a plug-and-play training paradigm Solution-Guidance Fine-Tuning (SGFT) for enhancing the reasoning capabilities of small language models. SG focuses on problem understanding and decomposition at the semantic and logical levels, rather than specific computations, which can effectively improve the SLMs' generalization and reasoning abilities. With only a small amount of SG training data, SGFT can fine-tune a SLM to produce accurate problem-solving guidances, which can then be flexibly fed to any SLM as prompts, enabling it to generate correct answers directly. Experimental results demonstrate that our method significantly improves the performance of SLMs on various reasoning tasks, enhancing both their practicality and efficiency within resource-constrained environments.

### code
The code and key data will be available soon!
